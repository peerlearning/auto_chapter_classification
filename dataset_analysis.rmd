---
title: "Dataset Analysis"
output: html_document
---

```{r setup, include=FALSE}
library('tidyverse')
library('e1071')
library('SparseM')
library('tm')
library('caret')
#library('SnowballC')
#library('wordcloud')
library('jsonlite')

library(reticulate)
path_to_python <- "/usr/local/bin/python3"
use_python(path_to_python)
knitr::knit_engines$set(python = reticulate::eng_python)
knitr::opts_chunk$set(echo = TRUE)
py_available(initialize = TRUE)
```

## Import our dataset and apply basic cleaning

```{r import_dataset}
## Reading data from CMS
Qs <- jsonlite::fromJSON('qs_topicwise.json')
Qs <- flatten(Qs)

# Need to clean the overflow of text
Qs_clean <- Qs  %>%
  #dplyr::select(1:9) %>%                    # Keeping only the first 9 columns
  #dplyr::filter(!is.na(difficulty))  %>%    # No need to clean overflow
  dplyr::mutate(Grade = str_sub(topic_code, 5, 6), Subject = str_sub(topic_code, 1, 3), Curriculum = str_sub(topic_code, 8, 10), Ch_No = str_sub(topic_code, 12, 13))

repo <- Qs_clean %>% filter(Curriculum =="JEE", Subject =="CHM")

chapters_to_remove = c('Selection Test', 'Repository', 'Bridge Intervention Curriculum', 'M1.1 Scaffold test','Tally Marks')

chapters_with_no_data = c('')

repo <- repo %>% 
  filter(!chapter %in%  chapters_to_remove) %>%
  filter(!chapter %in%  chapters_with_no_data)

repo<-repo[sample(nrow(repo)),]       ## Randomize row numbers
text_corpus <- VCorpus(VectorSource(repo$question_text)) 
lapply(text_corpus[2:4], as.character)       # Multiple docs

chapters = repo %>% group_by(chapter) %>% summarize()
chapters = chapters$chapter

# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")

## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.

#text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))

# Remove one letter words
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern="\\b\\w{1,2}\\b", replacement = "")

## Add stopwords like left, right (frac ?)
text_corpus_clean <- tm_map(text_corpus_clean, removeWords, c(stopwords(), "left","right"))
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
#text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
text_corpus_clean <- tm_map(text_corpus_clean, stripWhitespace)

lapply(text_corpus_clean[2:4], as.character) 

```

## Create our document term matrix

```{r create_dtm}

# Algorithm. Choices: svm, naiveBayes, etc
algorithm = naiveBayes

# Categorical or not. Choices: TRUE or FALSE
categorical = TRUE

# Weighting function. Choices: weightTf, weightTfIdf
# Whether to normalize terms of Document Matrix. Choices: TRUE, FALSE
weightingFunction = weightTf
normalize = FALSE

# Whether to write to db
writeToDb = TRUE

text_dtm <- DocumentTermMatrix(text_corpus_clean, control = list(stemming = TRUE, weighting = weightingFunction, normalize = normalize))
```
## Getting most commonly used words in each chapter

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r get_common_words}

x = t(sapply(chapters, function(l) c(l, names(head(sort(row_sums(t(text_dtm[ which(repo$chapter == l), ])), decreasing = TRUE), 50)))))
df = as.data.frame(x)
```