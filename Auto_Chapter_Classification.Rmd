---
title: "R Notebook"
output: html_notebook
---
## Reading data from CMS

```{r message=FALSE, warning=FALSE, results='asis'}
library('tidyverse')
library('e1071')
library('SparseM')
library('tm')
library('caret')
#library('SnowballC')
library('wordcloud')
library('jsonlite')

Qs <- jsonlite::fromJSON('qs_topicwise.json')
Qs <- flatten(Qs)

#Qs <- read_tsv("qs_topicwise_dump.tsv")

head(Qs)
```

### Cleaning and adding Grade, Subject, Curriculum and Chapter No 

```{r results='asis'}
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
  dplyr::select(1:9) %>%                    # Keeping only the first 12 columns
  dplyr::filter(!is.na(difficulty))  %>%           # Cleaning overflow
  dplyr::mutate(Grade = str_sub(topic_code, 5, 6), Subject = str_sub(topic_code, 1, 3), Curriculum = str_sub(topic_code, 8, 10), Ch_No = str_sub(topic_code, 12, 13))

# Removing non-UTF characters
Qs_clean$Text <- lapply(Qs_clean$question_text,gsub, pattern = "[^[:alnum:]]", replacement = " ")
Qs_clean$question_text <- lapply(Qs_clean$question_text,gsub, pattern = "<.*?>", replacement= " ")

knitr::kable(head(Qs_clean))

```

### Summarizing Chapter wise # Qs in the entire JEE dataset

```{r}
Qs_test <- Qs_clean %>% filter(Curriculum =="JEE")

chapters_to_remove = c('Selection Test', 'Repository', 'Bridge Intervention Curriculum', 'M1.1 Scaffold test', 'M1.1 Scaffold test')

Qs_test <- Qs_test %>% filter(!chapter %in%  chapters_to_remove)

Qs_test %>% group_by(chapter) %>% summarize(count_qs =n())

```


### Picking chapters for classification from the CMS repo

```{r eval= TRUE}

# multi <-  filter(Qs_test, chapter %in% c("Aldehydes and Ketones", "Coordination Compounds", "Biomolecules, Polymers and Chemistry in Everyday life", "Electrochemistry", "Amines")) 
# Using chapter names for filtering gave the error message :
# longer object length is not a multiple of shorter object length
multi <- Qs_test
#multi <- arrange(multi, Code)      ## To rearrange and randomize chapters by row no

multi %>% group_by(chapter) %>% summarize(count_qs =n())

```

### Changing Chapter labels to factors

The Chapter variable is currently a character vector. Since this is a categorical variable, it would be better to convert it to a factor.
```{r}
multi$chapter <- factor(multi$chapter)
str(multi$chapter)
```

## Create text corpus

```{r}

multi<-multi[sample(nrow(multi)),]
text_corpus <- VCorpus(VectorSource(multi$question_text)) 
##include both and test and training set to build the corpus
#inspect (text_corpus)
```

### Let's view some Qs
```{r}
lapply(text_corpus[2:4], as.character)       # Multiple docs
```

## Cleaning text 
Removing punctuations, numbers and stop words
Converting to lower case
Stemming words - learned, learning, and learns are transformed into the base form, learn
Removing additional white spaces

```{r}
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")

## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.

text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))

## Add stopwords like left, right (frac ?)
text_corpus_clean <- tm_map(text_corpus_clean, removeWords, c(stopwords(), "left","right"))
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
#text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
text_corpus_clean <- tm_map(text_corpus_clean, stripWhitespace)
```

### Let's view some cleaned Qs
```{r}
lapply(text_corpus_clean[2:4], as.character)       
```

## Bag of words - Tokenization
```{r}
text_dtm <- DocumentTermMatrix(text_corpus_clean)
text_dtm

# Add a dictionary to DTM ?
```

## Data preparation - Creating training and test datasets

```{r}

folds <- cut(seq(1,nrow(multi)),breaks=5,labels=FALSE)
test_indices = which(folds==1, arr.ind=TRUE)

text_dtm_train <- text_dtm[-test_indices,  ]

text_dtm_test <- text_dtm[test_indices,  ]


text_train_labels <- multi[-test_indices,  ]$chapter
text_test_labels <- multi[test_indices,  ]$chapter 

prop.table(table(text_train_labels))
```

### Distribution of labels in test set
```{r}
prop.table(table(text_test_labels))
```

## Visualizing text data - word clouds
```{r}
# wordcloud(text_corpus_clean, min.freq=10, random.order = FALSE)
```

### Word Cloud from chapter 1
```{r}
# wordcloud(text_corpus_clean[multi$chapter == "Applications of Derivatives"], max.words = 40, scale = c(3, 0.5))
```

### Word cloud from the chapter 2
```{r echo=FALSE, eval=TRUE}
# wordcloud(text_corpus_clean[multi$Chapter == "Fundamentals of Mathematics"], max.words = 40, scale = c(3, 0.5))
```
### Word cloud from Chapter 3
```{r eval=TRUE}
# wordcloud(text_corpus_clean[multi$Chapter == "Conic Sections - I"], max.words = 40, scale = c(3, 0.5))
```

Words appearing at least a specified number of times.Filter our DTM to include only the terms appearing in a specified vector.

```{r}
text_freq_words <- findFreqTerms(text_dtm, 25)

# tf-idf ?
# Useful to remove words which maybe in training set but not in test set leading to an out of bounds error

text_dtm_freq_train <- text_dtm_train[ , text_freq_words]
text_dtm_freq_test <- text_dtm_test[ , text_freq_words]
```

The Naive Bayes classifier is typically trained on data with categorical features. This poses a problem, since the cells in the sparse matrix are numeric and measure the number of times a word appears in a message. We need to change this to a categorical variable that simply indicates yes or no depending on whether the word appears at all. Train matrix :

```{r}
convert_counts <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}

# MARGIN = 1 is used for rows
text_train <- apply(text_dtm_freq_train, MARGIN = 2, convert_counts)
text_test <- apply(text_dtm_freq_test, MARGIN = 2, convert_counts)
str(text_train)
```

Test matrix :
```{r}
str(text_test)
```


## TRAIN NAIVE BAYES MODEL 
```{r}
text_classifier <- naiveBayes(text_train, text_train_labels, laplace = 1)

# Laplace - This allows words that did not appear earlier to have an indisputable say in the classification process. Just because the word "ringtone" only appeared in the spam messages in the training data, it does not mean that every message with this word should be classified as spam
```

## PREDICTION
```{r}
#results <- predict(text_classifier,as.matrix(text_dtm_test))
results <- predict(text_classifier,text_test)
true_values <- multi[test_indices, ]$chapter

conf = confusionMatrix(results, text_test_labels)
confusion_table = as.data.frame.matrix( conf$table  )
conf_stats = as.data.frame(conf$byClass)
confusion_table[nrow(confusion_table) + 1, ] <- conf_stats$Sensitivity
q_counts = multi %>% group_by(chapter) %>% summarize(count_qs =n())
confusion_table[nrow(confusion_table) + 1, ] <- t(q_counts[,2])

#View(results)
```

### GETTING TOP 5 STATS

```{r}


  df.frame <- as.data.frame(matrix(ncol=7, nrow=nrow(results)))
  
  labels = colnames(results)
  for (i in 1:nrow(results)) {
    true_value = true_values[i]
    probs = results[i, ]
    goodPred = FALSE
    preds = vector()
    pred_probs = vector()
    ind = which.max(probs)
    prediction = labels[ind]
    for (j in 1:5) {
      ind = which.max(probs)
      prob = max(probs)
      probs <- probs[-ind]
      x <- match(prob, results[i, ])
    preds <- c(preds, labels[x])
     pred_probs <- c(pred_probs, prob)
   }
   if (labels[true_value] %in% prediction) {
     goodPred = TRUE
   }
   df.frame[i, ] = c(labels[true_value], goodPred, pred_probs)
 }

 top_five_stats = df.frame %>% group_by(V2) %>% summarize(n())
 top_five_accuracy = top_five_stats[2,2] / (top_five_stats[1,2] + top_five_stats[2,2])

```

