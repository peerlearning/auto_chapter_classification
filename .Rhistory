}
mat = matrix(ncol = 51)
nrow(levels)
nrow(all_labels)
ncol(all_labels)
size(all_labels)
count(all_labels)
all_labels
length(all_labels)
x = t(sapply(label in all_labels, function(l) c(l, tail(sort(row_sums(t(text_dtm[ which(repo$chapter == l), ]))), 50))))
x = t(sapply(all_labels, function(l) c(l, tail(sort(row_sums(t(text_dtm[ which(repo$chapter == l), ]))), 50))))
x
x = t(sapply(all_labels, function(l) c(l, names(tail(sort(row_sums(t(text_dtm[ which(repo$chapter == l), ]))), 50)))))
x
counts_df = as.data.frame(x)
View(counts_df)
x = t(sapply(all_labels, function(l) c(l, names(head(sort(row_sums(t(text_dtm[ which(repo$chapter == l), ])), decreasing = TRUE), 50)))))
counts_df = as.data.frame(x)
View(counts_df)
repo %>% group_by(chapter)
repo %>% group_by(chapter) %>% summarize()
repo %>% group_by(chapter) %>% summarize()$chapter
x = repo %>% group_by(chapter) %>% summarize()
x
x$chapter
library('tidyverse')
library('e1071')
library('SparseM')
library('tm')
library('caret')
#library('SnowballC')
#library('wordcloud')
library('jsonlite')
library(reticulate)
path_to_python <- "/usr/local/bin/python3"
use_python(path_to_python)
knitr::knit_engines$set(python = reticulate::eng_python)
knitr::opts_chunk$set(echo = TRUE)
py_available(initialize = TRUE)
## Reading data from CMS
Qs <- jsonlite::fromJSON('qs_topicwise.json')
Qs <- flatten(Qs)
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
#dplyr::select(1:9) %>%                    # Keeping only the first 9 columns
#dplyr::filter(!is.na(difficulty))  %>%    # No need to clean overflow
dplyr::mutate(Grade = str_sub(topic_code, 5, 6), Subject = str_sub(topic_code, 1, 3), Curriculum = str_sub(topic_code, 8, 10), Ch_No = str_sub(topic_code, 12, 13))
repo <- Qs_clean %>% filter(Curriculum =="JEE", Subject =="MTH")
chapters_to_remove = c('Selection Test', 'Repository', 'Bridge Intervention Curriculum', 'M1.1 Scaffold test','Tally Marks')
chapters_with_no_data = c('Static Electricity')
repo <- repo %>%
filter(!chapter %in%  chapters_to_remove) %>%
filter(!chapter %in%  chapters_with_no_data)
repo<-repo[sample(nrow(repo)),]       ## Randomize row numbers
text_corpus <- VCorpus(VectorSource(repo$question_text))
lapply(text_corpus[2:4], as.character)       # Multiple docs
chapters = repo %>% group_by(chapter) %>% summarize()
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
#text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))
# Remove one letter words
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern="\\b\\w{1,2}\\b", replacement = "")
## Add stopwords like left, right (frac ?)
text_corpus_clean <- tm_map(text_corpus_clean, removeWords, c(stopwords(), "left","right"))
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
#text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
text_corpus_clean <- tm_map(text_corpus_clean, stripWhitespace)
lapply(text_corpus_clean[2:4], as.character)
# Algorithm. Choices: svm, naiveBayes, etc
algorithm = naiveBayes
# Categorical or not. Choices: TRUE or FALSE
categorical = TRUE
# Weighting function. Choices: weightTf, weightTfIdf
# Whether to normalize terms of Document Matrix. Choices: TRUE, FALSE
weightingFunction = weightTf
normalize = FALSE
# Whether to write to db
writeToDb = TRUE
text_dtm <- DocumentTermMatrix(text_corpus_clean, control = list(weighting = weightingFunction, normalize = normalize))
x = t(sapply(chapters, function(l) c(l, names(head(sort(row_sums(t(text_dtm[ which(repo$chapter == l), ])), decreasing = TRUE), 50)))))
df = as.data.frame(x)
chapters
library('tidyverse')
library('e1071')
library('SparseM')
library('tm')
library('caret')
#library('SnowballC')
#library('wordcloud')
library('jsonlite')
library(reticulate)
path_to_python <- "/usr/local/bin/python3"
use_python(path_to_python)
knitr::knit_engines$set(python = reticulate::eng_python)
knitr::opts_chunk$set(echo = TRUE)
py_available(initialize = TRUE)
## Reading data from CMS
Qs <- jsonlite::fromJSON('qs_topicwise.json')
Qs <- flatten(Qs)
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
#dplyr::select(1:9) %>%                    # Keeping only the first 9 columns
#dplyr::filter(!is.na(difficulty))  %>%    # No need to clean overflow
dplyr::mutate(Grade = str_sub(topic_code, 5, 6), Subject = str_sub(topic_code, 1, 3), Curriculum = str_sub(topic_code, 8, 10), Ch_No = str_sub(topic_code, 12, 13))
repo <- Qs_clean %>% filter(Curriculum =="JEE", Subject =="MTH")
chapters_to_remove = c('Selection Test', 'Repository', 'Bridge Intervention Curriculum', 'M1.1 Scaffold test','Tally Marks')
chapters_with_no_data = c('Static Electricity')
repo <- repo %>%
filter(!chapter %in%  chapters_to_remove) %>%
filter(!chapter %in%  chapters_with_no_data)
repo<-repo[sample(nrow(repo)),]       ## Randomize row numbers
text_corpus <- VCorpus(VectorSource(repo$question_text))
lapply(text_corpus[2:4], as.character)       # Multiple docs
chapters = repo %>% group_by(chapter) %>% summarize()
chapters = chapters$chapter
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
#text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))
# Remove one letter words
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern="\\b\\w{1,2}\\b", replacement = "")
## Add stopwords like left, right (frac ?)
text_corpus_clean <- tm_map(text_corpus_clean, removeWords, c(stopwords(), "left","right"))
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
#text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
text_corpus_clean <- tm_map(text_corpus_clean, stripWhitespace)
lapply(text_corpus_clean[2:4], as.character)
# Algorithm. Choices: svm, naiveBayes, etc
algorithm = naiveBayes
# Categorical or not. Choices: TRUE or FALSE
categorical = TRUE
# Weighting function. Choices: weightTf, weightTfIdf
# Whether to normalize terms of Document Matrix. Choices: TRUE, FALSE
weightingFunction = weightTf
normalize = FALSE
# Whether to write to db
writeToDb = TRUE
text_dtm <- DocumentTermMatrix(text_corpus_clean, control = list(weighting = weightingFunction, normalize = normalize))
x = t(sapply(chapters, function(l) c(l, names(head(sort(row_sums(t(text_dtm[ which(repo$chapter == l), ])), decreasing = TRUE), 50)))))
df = as.data.frame(x)
View(df)
library('tidyverse')
library('e1071')
library('SparseM')
library('tm')
library('caret')
#library('SnowballC')
#library('wordcloud')
library('jsonlite')
library(reticulate)
path_to_python <- "/usr/local/bin/python3"
use_python(path_to_python)
knitr::knit_engines$set(python = reticulate::eng_python)
knitr::opts_chunk$set(echo = TRUE)
py_available(initialize = TRUE)
## Reading data from CMS
Qs <- jsonlite::fromJSON('qs_topicwise.json')
Qs <- flatten(Qs)
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
#dplyr::select(1:9) %>%                    # Keeping only the first 9 columns
#dplyr::filter(!is.na(difficulty))  %>%    # No need to clean overflow
dplyr::mutate(Grade = str_sub(topic_code, 5, 6), Subject = str_sub(topic_code, 1, 3), Curriculum = str_sub(topic_code, 8, 10), Ch_No = str_sub(topic_code, 12, 13))
repo <- Qs_clean %>% filter(Curriculum =="JEE", Subject =="PHY")
chapters_to_remove = c('Selection Test', 'Repository', 'Bridge Intervention Curriculum', 'M1.1 Scaffold test','Tally Marks')
repo <- repo %>%
filter(!chapter %in%  chapters_to_remove) %>%
filter(!chapter %in%  chapters_with_no_data)
library('tidyverse')
library('e1071')
library('SparseM')
library('tm')
library('caret')
#library('SnowballC')
#library('wordcloud')
library('jsonlite')
library(reticulate)
path_to_python <- "/usr/local/bin/python3"
use_python(path_to_python)
knitr::knit_engines$set(python = reticulate::eng_python)
knitr::opts_chunk$set(echo = TRUE)
py_available(initialize = TRUE)
## Reading data from CMS
Qs <- jsonlite::fromJSON('qs_topicwise.json')
Qs <- flatten(Qs)
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
#dplyr::select(1:9) %>%                    # Keeping only the first 9 columns
#dplyr::filter(!is.na(difficulty))  %>%    # No need to clean overflow
dplyr::mutate(Grade = str_sub(topic_code, 5, 6), Subject = str_sub(topic_code, 1, 3), Curriculum = str_sub(topic_code, 8, 10), Ch_No = str_sub(topic_code, 12, 13))
repo <- Qs_clean %>% filter(Curriculum =="JEE", Subject =="PHY")
chapters_to_remove = c('Selection Test', 'Repository', 'Bridge Intervention Curriculum', 'M1.1 Scaffold test','Tally Marks')
chapters_with_no_data = c('')
repo <- repo %>%
filter(!chapter %in%  chapters_to_remove) %>%
filter(!chapter %in%  chapters_with_no_data)
repo<-repo[sample(nrow(repo)),]       ## Randomize row numbers
text_corpus <- VCorpus(VectorSource(repo$question_text))
lapply(text_corpus[2:4], as.character)       # Multiple docs
chapters = repo %>% group_by(chapter) %>% summarize()
chapters = chapters$chapter
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
#text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))
# Remove one letter words
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern="\\b\\w{1,2}\\b", replacement = "")
## Add stopwords like left, right (frac ?)
text_corpus_clean <- tm_map(text_corpus_clean, removeWords, c(stopwords(), "left","right"))
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
#text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
text_corpus_clean <- tm_map(text_corpus_clean, stripWhitespace)
lapply(text_corpus_clean[2:4], as.character)
# Algorithm. Choices: svm, naiveBayes, etc
algorithm = naiveBayes
# Categorical or not. Choices: TRUE or FALSE
categorical = TRUE
# Weighting function. Choices: weightTf, weightTfIdf
# Whether to normalize terms of Document Matrix. Choices: TRUE, FALSE
weightingFunction = weightTf
normalize = FALSE
# Whether to write to db
writeToDb = TRUE
text_dtm <- DocumentTermMatrix(text_corpus_clean, control = list(weighting = weightingFunction, normalize = normalize))
x = t(sapply(chapters, function(l) c(l, names(head(sort(row_sums(t(text_dtm[ which(repo$chapter == l), ])), decreasing = TRUE), 50)))))
df = as.data.frame(x)
View(df)
library('tidyverse')
library('e1071')
library('SparseM')
library('tm')
library('caret')
#library('SnowballC')
#library('wordcloud')
library('jsonlite')
library(reticulate)
path_to_python <- "/usr/local/bin/python3"
use_python(path_to_python)
knitr::knit_engines$set(python = reticulate::eng_python)
knitr::opts_chunk$set(echo = TRUE)
py_available(initialize = TRUE)
## Reading data from CMS
Qs <- jsonlite::fromJSON('qs_topicwise.json')
Qs <- flatten(Qs)
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
#dplyr::select(1:9) %>%                    # Keeping only the first 9 columns
#dplyr::filter(!is.na(difficulty))  %>%    # No need to clean overflow
dplyr::mutate(Grade = str_sub(topic_code, 5, 6), Subject = str_sub(topic_code, 1, 3), Curriculum = str_sub(topic_code, 8, 10), Ch_No = str_sub(topic_code, 12, 13))
repo <- Qs_clean %>% filter(Curriculum =="JEE", Subject =="CHM")
chapters_to_remove = c('Selection Test', 'Repository', 'Bridge Intervention Curriculum', 'M1.1 Scaffold test','Tally Marks')
chapters_with_no_data = c('')
repo <- repo %>%
filter(!chapter %in%  chapters_to_remove) %>%
filter(!chapter %in%  chapters_with_no_data)
repo<-repo[sample(nrow(repo)),]       ## Randomize row numbers
text_corpus <- VCorpus(VectorSource(repo$question_text))
lapply(text_corpus[2:4], as.character)       # Multiple docs
chapters = repo %>% group_by(chapter) %>% summarize()
chapters = chapters$chapter
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
#text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))
# Remove one letter words
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern="\\b\\w{1,2}\\b", replacement = "")
## Add stopwords like left, right (frac ?)
text_corpus_clean <- tm_map(text_corpus_clean, removeWords, c(stopwords(), "left","right"))
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
#text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
text_corpus_clean <- tm_map(text_corpus_clean, stripWhitespace)
lapply(text_corpus_clean[2:4], as.character)
# Algorithm. Choices: svm, naiveBayes, etc
algorithm = naiveBayes
# Categorical or not. Choices: TRUE or FALSE
categorical = TRUE
# Weighting function. Choices: weightTf, weightTfIdf
# Whether to normalize terms of Document Matrix. Choices: TRUE, FALSE
weightingFunction = weightTf
normalize = FALSE
# Whether to write to db
writeToDb = TRUE
text_dtm <- DocumentTermMatrix(text_corpus_clean, control = list(weighting = weightingFunction, normalize = normalize))
x = t(sapply(chapters, function(l) c(l, names(head(sort(row_sums(t(text_dtm[ which(repo$chapter == l), ])), decreasing = TRUE), 50)))))
df = as.data.frame(x)
View(df)
library('SnowballC')
wordStem(text_dtm)
library('tidyverse')
library('e1071')
library('SparseM')
library('tm')
library('caret')
#library('SnowballC')
#library('wordcloud')
library('jsonlite')
library(reticulate)
path_to_python <- "/usr/local/bin/python3"
use_python(path_to_python)
knitr::knit_engines$set(python = reticulate::eng_python)
py_available(initialize = TRUE)
# Algorithm. Choices: svm, naiveBayes, etc
algorithm = naiveBayes
# Categorical or not. Choices: TRUE or FALSE
categorical = TRUE
# Weighting function. Choices: weightTf, weightTfIdf
# Whether to normalize terms of Document Matrix. Choices: TRUE, FALSE
weightingFunction = weightTf
normalize = FALSE
# Whether to write to db
writeToDb = TRUE
## Reading data from CMS
Qs <- jsonlite::fromJSON('qs_topicwise.json')
Qs <- flatten(Qs)
#Qs <- read_tsv("qs_topicwise_dump.tsv")
head(Qs)
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
#dplyr::select(1:9) %>%                    # Keeping only the first 9 columns
#dplyr::filter(!is.na(difficulty))  %>%    # No need to clean overflow
dplyr::mutate(Grade = str_sub(topic_code, 5, 6), Subject = str_sub(topic_code, 1, 3), Curriculum = str_sub(topic_code, 8, 10), Ch_No = str_sub(topic_code, 12, 13))
repo <- Qs_clean %>% filter(Curriculum =="JEE", Subject =="MTH")
chapters_to_remove = c('Selection Test', 'Repository', 'Bridge Intervention Curriculum', 'M1.1 Scaffold test','Tally Marks')
chapters_with_no_data = c('Static Electricity')
repo <- repo %>%
filter(!chapter %in%  chapters_to_remove) %>%
filter(!chapter %in%  chapters_with_no_data)
repo %>% group_by(chapter) %>% summarize(count_qs =n())
repo<-repo[sample(nrow(repo)),]       ## Randomize row numbers
text_corpus <- VCorpus(VectorSource(repo$question_text))
lapply(text_corpus[2:4], as.character)       # Multiple docs
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
#text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))
# Remove one letter words
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern="\\b\\w{1,2}\\b", replacement = "")
## Add stopwords like left, right (frac ?)
#text_corpus_clean <- tm_map(text_corpus_clean, removeWords, c(stopwords(), "left","right"))
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
#text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
text_corpus_clean <- tm_map(text_corpus_clean, stripWhitespace)
lapply(text_corpus_clean[2:4], as.character)
# TF-IDF
text_dtm <- DocumentTermMatrix(text_corpus_clean, control = list(stemming = TRUE, weighting = weightingFunction, normalize = normalize))
# Add a dictionary to DTM ?
folds <- cut(seq(1,nrow(repo)),breaks=5,labels=FALSE)
test_indices = which(folds==1, arr.ind=TRUE)
text_dtm_train <- text_dtm[-test_indices,  ]
text_dtm_test <- text_dtm[test_indices,  ]
text_train_labels <- as.factor(repo[-test_indices,  ]$chapter)
text_test_labels <- as.factor(repo[test_indices,  ]$chapter)
round(prop.table(table(text_train_labels))*100,2)
round(prop.table(table(text_test_labels))*100,2)
## Filter features by selecting words appearing at least a specified number of times
text_freq_words <- findFreqTerms(text_dtm, 25)
# tf-idf ?
# Useful to remove words which maybe in training set but not in test set leading to an out of bounds error
text_dtm_freq_train <- text_dtm_train[ , text_freq_words]
text_dtm_freq_test <- text_dtm_test[ , text_freq_words]
convert_counts <- function(x) {
if(categorical) {
x <- ifelse(x > 0, "Yes", "No")
}
else {
x <- x
}
}
# MARGIN = 1 is used for rows
text_train <- apply(text_dtm_freq_train, MARGIN = 2, convert_counts)
text_test <- apply(text_dtm_freq_test, MARGIN = 2, convert_counts)
# Laplace - This allows words that did not appear earlier to have an indisputable say in the classification process. Just because the word "ringtone" only appeared in the spam messages in the training data, it does not mean that every message with this word should be classified as spam
text_classifier <- algorithm(as.matrix(text_train), text_train_labels)
rawResults <- predict(text_classifier,as.matrix(text_test), "raw")
results <- as.factor( colnames(rawResults)[apply(rawResults, 1, which.max)] )
true_values <- repo[test_indices, ]$chapter
test_document <- repo[test_indices, ]$question_text
## as.table((test_document,true_values, results)) ??
conf = confusionMatrix(results, text_test_labels)
library('tidyverse')
library('e1071')
library('SparseM')
library('tm')
library('caret')
#library('SnowballC')
#library('wordcloud')
library('jsonlite')
library(reticulate)
path_to_python <- "/usr/local/bin/python3"
use_python(path_to_python)
knitr::knit_engines$set(python = reticulate::eng_python)
knitr::opts_chunk$set(echo = TRUE)
py_available(initialize = TRUE)
## Reading data from CMS
Qs <- jsonlite::fromJSON('qs_topicwise.json')
Qs <- flatten(Qs)
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
#dplyr::select(1:9) %>%                    # Keeping only the first 9 columns
#dplyr::filter(!is.na(difficulty))  %>%    # No need to clean overflow
dplyr::mutate(Grade = str_sub(topic_code, 5, 6), Subject = str_sub(topic_code, 1, 3), Curriculum = str_sub(topic_code, 8, 10), Ch_No = str_sub(topic_code, 12, 13))
repo <- Qs_clean %>% filter(Curriculum =="JEE", Subject =="CHM")
chapters_to_remove = c('Selection Test', 'Repository', 'Bridge Intervention Curriculum', 'M1.1 Scaffold test','Tally Marks')
chapters_with_no_data = c('')
repo <- repo %>%
filter(!chapter %in%  chapters_to_remove) %>%
filter(!chapter %in%  chapters_with_no_data)
repo<-repo[sample(nrow(repo)),]       ## Randomize row numbers
text_corpus <- VCorpus(VectorSource(repo$question_text))
lapply(text_corpus[2:4], as.character)       # Multiple docs
chapters = repo %>% group_by(chapter) %>% summarize()
chapters = chapters$chapter
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
#text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))
# Remove one letter words
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern="\\b\\w{1,2}\\b", replacement = "")
## Add stopwords like left, right (frac ?)
text_corpus_clean <- tm_map(text_corpus_clean, removeWords, c(stopwords(), "left","right"))
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
#text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
text_corpus_clean <- tm_map(text_corpus_clean, stripWhitespace)
lapply(text_corpus_clean[2:4], as.character)
# Algorithm. Choices: svm, naiveBayes, etc
algorithm = naiveBayes
# Categorical or not. Choices: TRUE or FALSE
categorical = TRUE
# Weighting function. Choices: weightTf, weightTfIdf
# Whether to normalize terms of Document Matrix. Choices: TRUE, FALSE
weightingFunction = weightTf
normalize = FALSE
# Whether to write to db
writeToDb = TRUE
text_dtm <- DocumentTermMatrix(text_corpus_clean, control = list(temming = TRUE, weighting = weightingFunction, normalize = normalize))
x = t(sapply(chapters, function(l) c(l, names(head(sort(row_sums(t(text_dtm[ which(repo$chapter == l), ])), decreasing = TRUE), 50)))))
df = as.data.frame(x)
View(df)
library('tidyverse')
library('e1071')
library('SparseM')
library('tm')
library('caret')
#library('SnowballC')
#library('wordcloud')
library('jsonlite')
library(reticulate)
path_to_python <- "/usr/local/bin/python3"
use_python(path_to_python)
knitr::knit_engines$set(python = reticulate::eng_python)
knitr::opts_chunk$set(echo = TRUE)
py_available(initialize = TRUE)
## Reading data from CMS
Qs <- jsonlite::fromJSON('qs_topicwise.json')
Qs <- flatten(Qs)
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
#dplyr::select(1:9) %>%                    # Keeping only the first 9 columns
#dplyr::filter(!is.na(difficulty))  %>%    # No need to clean overflow
dplyr::mutate(Grade = str_sub(topic_code, 5, 6), Subject = str_sub(topic_code, 1, 3), Curriculum = str_sub(topic_code, 8, 10), Ch_No = str_sub(topic_code, 12, 13))
repo <- Qs_clean %>% filter(Curriculum =="JEE", Subject =="CHM")
chapters_to_remove = c('Selection Test', 'Repository', 'Bridge Intervention Curriculum', 'M1.1 Scaffold test','Tally Marks')
chapters_with_no_data = c('')
repo <- repo %>%
filter(!chapter %in%  chapters_to_remove) %>%
filter(!chapter %in%  chapters_with_no_data)
repo<-repo[sample(nrow(repo)),]       ## Randomize row numbers
text_corpus <- VCorpus(VectorSource(repo$question_text))
lapply(text_corpus[2:4], as.character)       # Multiple docs
chapters = repo %>% group_by(chapter) %>% summarize()
chapters = chapters$chapter
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
#text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))
# Remove one letter words
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern="\\b\\w{1,2}\\b", replacement = "")
## Add stopwords like left, right (frac ?)
text_corpus_clean <- tm_map(text_corpus_clean, removeWords, c(stopwords(), "left","right"))
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
#text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
text_corpus_clean <- tm_map(text_corpus_clean, stripWhitespace)
lapply(text_corpus_clean[2:4], as.character)
# Algorithm. Choices: svm, naiveBayes, etc
algorithm = naiveBayes
# Categorical or not. Choices: TRUE or FALSE
categorical = TRUE
# Weighting function. Choices: weightTf, weightTfIdf
# Whether to normalize terms of Document Matrix. Choices: TRUE, FALSE
weightingFunction = weightTf
normalize = FALSE
# Whether to write to db
writeToDb = TRUE
text_dtm <- DocumentTermMatrix(text_corpus_clean, control = list(stemming = TRUE, weighting = weightingFunction, normalize = normalize))
x = t(sapply(chapters, function(l) c(l, names(head(sort(row_sums(t(text_dtm[ which(repo$chapter == l), ])), decreasing = TRUE), 50)))))
df = as.data.frame(x)
View(df)
