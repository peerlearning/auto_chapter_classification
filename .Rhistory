test_document <- repo[test_indices, ]$question_text
## as.table((test_document,true_values, results)) ??
conf = confusionMatrix(rawResults, text_test_labels)
confusion_table = as.data.frame.matrix( conf$table  )
conf_stats = as.data.frame(conf$byClass)
confusion_table[nrow(confusion_table) + 1, ] <- conf_stats$Sensitivity
q_counts = repo %>% group_by(chapter) %>% summarize(count_qs =n())
confusion_table[nrow(confusion_table) + 1, ] <- t(q_counts[,2])
conf$overall
#View(conf_stats)
#View(round(confusion_table, 1))
text_train
library(class)
knn.pred <- knn(text_train, text_test, text_train_labels)
knn.pred
conf = confusionMatrix(knn.pred, text_test_labels)
conf
knn.pred <- knn(text_train, text_test, text_train_labels, k = 10)
conf = confusionMatrix(knn.pred, text_test_labels)
conf
knn.pred <- knn(text_train, text_test, text_train_labels)
categorical
library(bnlearn)
install.packages(bnlearn)
install.packages('bnlearn')
library(bnlearn)
nb.net <- naive.bayes(text_train, text_train_labels, "NLP")
text_train_labels
nb.net <- naive.bayes(text_train, as.vector(text_train_labels), "NLP")
nb.net <- naive.bayes(c(text_train as.vector(text_train_labels)), "NLP")
nb.net <- naive.bayes(c(text_train, as.vector(text_train_labels)), "NLP")
nb.net <- naive.bayes(as.data.frame(c(text_train, as.vector(text_train_labels))), "NLP")
as.data.frame(text_train)
library('tidyverse')
library('e1071')
library('SparseM')
library('tm')
library('caret')
#library('SnowballC')
#library('wordcloud')
library('jsonlite')
library(reticulate)
path_to_python <- "/usr/local/bin/python3"
use_python(path_to_python)
knitr::knit_engines$set(python = reticulate::eng_python)
py_available(initialize = TRUE)
# Algorithm. Choices: svm, naiveBayes, etc
algorithm = naiveBayes
# Categorical or not. Choices: TRUE or FALSE
categorical = FALSE
# Weighting function. Choices: weightTf, weightTfIdf
# Whether to normalize terms of Document Matrix. Choices: TRUE, FALSE
weightingFunction = weightTf
normalize = FALSE
# Whether to write to db
writeToDb = TRUE
subject = "MTH"
words_to_remove = vector(mode="list", length=3)
names(words_to_remove) = c("PHY", "CHM", "MTH")
# MATH
words_to_remove[[1]] =  c("rightarrow", "hence", "frac", "text", "sqrt", "times", "value", "amp", "statement", "will", "equal", "number", "tan", "now", "can", "two", "get", "true", "lambda")
# CHM
words_to_remove[[2]] = c("following", "can", "correct", "hence", "amp", "text", "will", "reaction", "given", "frac", "rightarrow", "circ", "number", "matrix", "mol", "compound", "true", "one", "group", "solution")
# # PHY
words_to_remove[[3]] = c("frac", "given", "times", "will", "two", "rightarrow", "sqrt")
subject_words_to_remove = c(stopwords(), get(subject, words_to_remove))
## Reading data from CMS
Qs <- jsonlite::fromJSON('data/qs_topicwise.json')
Qs <- flatten(Qs)
#Qs <- read_tsv("qs_topicwise_dump.tsv")
head(Qs)
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
#dplyr::select(1:9) %>%                    # Keeping only the first 9 columns
#dplyr::filter(!is.na(difficulty))  %>%    # No need to clean overflow
dplyr::mutate(Grade = str_sub(topic_code, 5, 6), Subject = str_sub(topic_code, 1, 3), Curriculum = str_sub(topic_code, 8, 10), Ch_No = str_sub(topic_code, 12, 13))
repo <- Qs_clean %>% filter(Curriculum =="JEE", Subject == subject)
chapters_to_remove = c('Selection Test', 'Repository', 'Bridge Intervention Curriculum', 'M1.1 Scaffold test','Tally Marks')
chapters_with_no_data = c('Static Electricity', 'Experimental Skills', 'Principle of Mathematical Induction')
repo <- repo %>%
filter(!chapter %in%  chapters_to_remove) %>%
filter(!chapter %in%  chapters_with_no_data)
dummy_questions = which(grepl("DUMMY", repo$question_text))
repo <- repo[-dummy_questions]
repo %>% group_by(chapter) %>% summarize(count_qs =n())
repo<-repo[sample(nrow(repo)),]       ## Randomize row numbers
text_corpus <- VCorpus(VectorSource(repo$question_text))
lapply(text_corpus[2:4], as.character)       # Multiple docs
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
#text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))
# Remove one letter words
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern="\\b\\w{1,2}\\b", replacement = "")
## Add stopwords like left, right (frac ?)
text_corpus_clean <- tm_map(text_corpus_clean, removeWords, subject_words_to_remove)
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
#text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
text_corpus_clean <- tm_map(text_corpus_clean, stripWhitespace)
lapply(text_corpus_clean[2:4], as.character)
# TF-IDF
text_dtm <- DocumentTermMatrix(text_corpus_clean, control = list(stemming = TRUE, weighting = weightingFunction, normalize = normalize))
# Add a dictionary to DTM ?
folds <- cut(seq(1,nrow(repo)),breaks=5,labels=FALSE)
test_indices = which(folds==1, arr.ind=TRUE)
text_dtm_train <- text_dtm[-test_indices,  ]
text_dtm_test <- text_dtm[test_indices,  ]
text_train_labels <- as.factor(repo[-test_indices,  ]$chapter)
text_test_labels <- as.factor(repo[test_indices,  ]$chapter)
round(prop.table(table(text_train_labels))*100,2)
round(prop.table(table(text_test_labels))*100,2)
## Filter features by selecting words appearing at least a specified number of times
text_freq_words <- findFreqTerms(text_dtm, 25)
# tf-idf ?
# Useful to remove words which maybe in training set but not in test set leading to an out of bounds error
text_dtm_freq_train <- text_dtm_train[ , text_freq_words]
text_dtm_freq_test <- text_dtm_test[ , text_freq_words]
convert_counts <- function(x) {
if(categorical) {
x <- ifelse(x > 0, "Yes", "No")
}
else {
x <- x
}
}
# MARGIN = 1 is used for rows
text_train <- apply(text_dtm_freq_train, MARGIN = 2, convert_counts)
text_test <- apply(text_dtm_freq_test, MARGIN = 2, convert_counts)
# Laplace - This allows words that did not appear earlier to have an indisputable say in the classification process. Just because the word "ringtone" only appeared in the spam messages in the training data, it does not mean that every message with this word should be classified as spam
text_classifier <- algorithm(as.matrix(text_train), text_train_labels)
rawResults <- predict(text_classifier,as.matrix(text_test))
# results <- as.factor( colnames(rawResults)[apply(rawResults, 1, which.max)] )
true_values <- repo[test_indices, ]$chapter
test_document <- repo[test_indices, ]$question_text
## as.table((test_document,true_values, results)) ??
conf = confusionMatrix(rawResults, text_test_labels)
confusion_table = as.data.frame.matrix( conf$table  )
conf_stats = as.data.frame(conf$byClass)
confusion_table[nrow(confusion_table) + 1, ] <- conf_stats$Sensitivity
q_counts = repo %>% group_by(chapter) %>% summarize(count_qs =n())
confusion_table[nrow(confusion_table) + 1, ] <- t(q_counts[,2])
conf$overall
#View(conf_stats)
#View(round(confusion_table, 1))
rawResults <- predict(text_classifier,as.matrix(text_test), type="raw")
# results <- as.factor( colnames(rawResults)[apply(rawResults, 1, which.max)] )
true_values <- repo[test_indices, ]$chapter
test_document <- repo[test_indices, ]$question_text
## as.table((test_document,true_values, results)) ??
rawResults
View(Qs_clean)
View(rawResults)
View(confusion_table)
View(confusion_table)
View(conf_stats)
library('tidyverse')
library('e1071')
library('SparseM')
library('tm')
library('caret')
#library('SnowballC')
#library('wordcloud')
library('jsonlite')
library(reticulate)
path_to_python <- "/usr/local/bin/python3"
use_python(path_to_python)
knitr::knit_engines$set(python = reticulate::eng_python)
py_available(initialize = TRUE)
# Algorithm. Choices: svm, naiveBayes, etc
algorithm = naiveBayes
# Categorical or not. Choices: TRUE or FALSE
categorical = FALSE
# Weighting function. Choices: weightTf, weightTfIdf
# Whether to normalize terms of Document Matrix. Choices: TRUE, FALSE
weightingFunction = weightTf
normalize = FALSE
# Whether to write to db
writeToDb = TRUE
subject = "PHY"
words_to_remove = vector(mode="list", length=3)
names(words_to_remove) = c("PHY", "CHM", "MTH")
# MATH
words_to_remove[[1]] =  c("rightarrow", "hence", "frac", "text", "sqrt", "times", "value", "amp", "statement", "will", "equal", "number", "tan", "now", "can", "two", "get", "true", "lambda")
# CHM
words_to_remove[[2]] = c("following", "can", "correct", "hence", "amp", "text", "will", "reaction", "given", "frac", "rightarrow", "circ", "number", "matrix", "mol", "compound", "true", "one", "group", "solution")
# # PHY
words_to_remove[[3]] = c("frac", "given", "times", "will", "two", "rightarrow", "sqrt")
subject_words_to_remove = c(stopwords(), get(subject, words_to_remove))
## Reading data from CMS
Qs <- jsonlite::fromJSON('data/qs_topicwise.json')
Qs <- flatten(Qs)
#Qs <- read_tsv("qs_topicwise_dump.tsv")
head(Qs)
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
#dplyr::select(1:9) %>%                    # Keeping only the first 9 columns
#dplyr::filter(!is.na(difficulty))  %>%    # No need to clean overflow
dplyr::mutate(Grade = str_sub(topic_code, 5, 6), Subject = str_sub(topic_code, 1, 3), Curriculum = str_sub(topic_code, 8, 10), Ch_No = str_sub(topic_code, 12, 13))
repo <- Qs_clean %>% filter(Curriculum =="JEE", Subject == subject)
chapters_to_remove = c('Selection Test', 'Repository', 'Bridge Intervention Curriculum', 'M1.1 Scaffold test','Tally Marks')
chapters_with_no_data = c('Static Electricity', 'Experimental Skills', 'Principle of Mathematical Induction')
repo <- repo %>%
filter(!chapter %in%  chapters_to_remove) %>%
filter(!chapter %in%  chapters_with_no_data)
dummy_questions = which(grepl("DUMMY", repo$question_text))
repo <- repo[-dummy_questions]
repo %>% group_by(chapter) %>% summarize(count_qs =n())
repo<-repo[sample(nrow(repo)),]       ## Randomize row numbers
text_corpus <- VCorpus(VectorSource(repo$question_text))
lapply(text_corpus[2:4], as.character)       # Multiple docs
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
#text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))
# Remove one letter words
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern="\\b\\w{1,2}\\b", replacement = "")
## Add stopwords like left, right (frac ?)
text_corpus_clean <- tm_map(text_corpus_clean, removeWords, subject_words_to_remove)
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
#text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
text_corpus_clean <- tm_map(text_corpus_clean, stripWhitespace)
lapply(text_corpus_clean[2:4], as.character)
# TF-IDF
text_dtm <- DocumentTermMatrix(text_corpus_clean, control = list(stemming = TRUE, weighting = weightingFunction, normalize = normalize))
# Add a dictionary to DTM ?
folds <- cut(seq(1,nrow(repo)),breaks=5,labels=FALSE)
test_indices = which(folds==1, arr.ind=TRUE)
text_dtm_train <- text_dtm[-test_indices,  ]
text_dtm_test <- text_dtm[test_indices,  ]
text_train_labels <- as.factor(repo[-test_indices,  ]$chapter)
text_test_labels <- as.factor(repo[test_indices,  ]$chapter)
round(prop.table(table(text_train_labels))*100,2)
round(prop.table(table(text_test_labels))*100,2)
## Filter features by selecting words appearing at least a specified number of times
text_freq_words <- findFreqTerms(text_dtm, 25)
# tf-idf ?
# Useful to remove words which maybe in training set but not in test set leading to an out of bounds error
text_dtm_freq_train <- text_dtm_train[ , text_freq_words]
text_dtm_freq_test <- text_dtm_test[ , text_freq_words]
convert_counts <- function(x) {
if(categorical) {
x <- ifelse(x > 0, "Yes", "No")
}
else {
x <- x
}
}
# MARGIN = 1 is used for rows
text_train <- apply(text_dtm_freq_train, MARGIN = 2, convert_counts)
text_test <- apply(text_dtm_freq_test, MARGIN = 2, convert_counts)
# Laplace - This allows words that did not appear earlier to have an indisputable say in the classification process. Just because the word "ringtone" only appeared in the spam messages in the training data, it does not mean that every message with this word should be classified as spam
text_classifier <- algorithm(as.matrix(text_train), text_train_labels)
rawResults <- predict(text_classifier,as.matrix(text_test), type="raw")
# results <- as.factor( colnames(rawResults)[apply(rawResults, 1, which.max)] )
true_values <- repo[test_indices, ]$chapter
test_document <- repo[test_indices, ]$question_text
## as.table((test_document,true_values, results)) ??
conf = confusionMatrix(rawResults, text_test_labels)
results <- as.factor( colnames(rawResults)[apply(rawResults, 1, which.max)] )
conf = confusionMatrix(results, text_test_labels)
results
conf
library('tidyverse')
library('e1071')
library('SparseM')
library('tm')
library('caret')
#library('SnowballC')
#library('wordcloud')
library('jsonlite')
library(reticulate)
path_to_python <- "/usr/local/bin/python3"
use_python(path_to_python)
knitr::knit_engines$set(python = reticulate::eng_python)
py_available(initialize = TRUE)
# Algorithm. Choices: svm, naiveBayes, etc
algorithm = naiveBayes
# Categorical or not. Choices: TRUE or FALSE
categorical = FALSE
# Weighting function. Choices: weightTf, weightTfIdf
# Whether to normalize terms of Document Matrix. Choices: TRUE, FALSE
weightingFunction = weightTf
normalize = FALSE
# Whether to write to db
writeToDb = TRUE
subject = "PHY"
words_to_remove = vector(mode="list", length=3)
names(words_to_remove) = c("PHY", "CHM", "MTH")
# MATH
words_to_remove[[1]] =  c("rightarrow", "hence", "frac", "text", "sqrt", "times", "value", "amp", "statement", "will", "equal", "number", "tan", "now", "can", "two", "get", "true", "lambda")
# CHM
words_to_remove[[2]] = c("following", "can", "correct", "hence", "amp", "text", "will", "reaction", "given", "frac", "rightarrow", "circ", "number", "matrix", "mol", "compound", "true", "one", "group", "solution")
# # PHY
words_to_remove[[3]] = c("frac", "given", "times", "will", "two", "rightarrow", "sqrt")
subject_words_to_remove = c(stopwords(), get(subject, words_to_remove))
## Reading data from CMS
Qs <- jsonlite::fromJSON('data/qs_topicwise.json')
Qs <- flatten(Qs)
#Qs <- read_tsv("qs_topicwise_dump.tsv")
head(Qs)
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
#dplyr::select(1:9) %>%                    # Keeping only the first 9 columns
#dplyr::filter(!is.na(difficulty))  %>%    # No need to clean overflow
dplyr::mutate(Grade = str_sub(topic_code, 5, 6), Subject = str_sub(topic_code, 1, 3), Curriculum = str_sub(topic_code, 8, 10), Ch_No = str_sub(topic_code, 12, 13))
repo <- Qs_clean %>% filter(Curriculum =="JEE", Subject == subject)
chapters_to_remove = c('Selection Test', 'Repository', 'Bridge Intervention Curriculum', 'M1.1 Scaffold test','Tally Marks')
chapters_with_no_data = c('Static Electricity', 'Experimental Skills', 'Principle of Mathematical Induction')
repo <- repo %>%
filter(!chapter %in%  chapters_to_remove) %>%
filter(!chapter %in%  chapters_with_no_data)
dummy_questions = which(grepl("DUMMY", repo$question_text))
repo <- repo[-dummy_questions]
repo %>% group_by(chapter) %>% summarize(count_qs =n())
repo<-repo[sample(nrow(repo)),]       ## Randomize row numbers
text_corpus <- VCorpus(VectorSource(repo$question_text))
lapply(text_corpus[2:4], as.character)       # Multiple docs
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
#text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))
# Remove one letter words
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern="\\b\\w{1,2}\\b", replacement = "")
## Add stopwords like left, right (frac ?)
text_corpus_clean <- tm_map(text_corpus_clean, removeWords, subject_words_to_remove)
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
#text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
text_corpus_clean <- tm_map(text_corpus_clean, stripWhitespace)
lapply(text_corpus_clean[2:4], as.character)
# TF-IDF
text_dtm <- DocumentTermMatrix(text_corpus_clean, control = list(stemming = TRUE)
# Add a dictionary to DTM ?
library('tidyverse')
library('e1071')
library('SparseM')
library('tm')
library('caret')
#library('SnowballC')
#library('wordcloud')
library('jsonlite')
library(reticulate)
path_to_python <- "/usr/local/bin/python3"
use_python(path_to_python)
knitr::knit_engines$set(python = reticulate::eng_python)
py_available(initialize = TRUE)
# Algorithm. Choices: svm, naiveBayes, etc
algorithm = naiveBayes
# Categorical or not. Choices: TRUE or FALSE
categorical = FALSE
# Weighting function. Choices: weightTf, weightTfIdf
# Whether to normalize terms of Document Matrix. Choices: TRUE, FALSE
weightingFunction = weightTf
normalize = FALSE
# Whether to write to db
writeToDb = TRUE
subject = "PHY"
words_to_remove = vector(mode="list", length=3)
names(words_to_remove) = c("PHY", "CHM", "MTH")
# MATH
words_to_remove[[1]] =  c("rightarrow", "hence", "frac", "text", "sqrt", "times", "value", "amp", "statement", "will", "equal", "number", "tan", "now", "can", "two", "get", "true", "lambda")
# CHM
words_to_remove[[2]] = c("following", "can", "correct", "hence", "amp", "text", "will", "reaction", "given", "frac", "rightarrow", "circ", "number", "matrix", "mol", "compound", "true", "one", "group", "solution")
# # PHY
words_to_remove[[3]] = c("frac", "given", "times", "will", "two", "rightarrow", "sqrt")
subject_words_to_remove = c(stopwords(), get(subject, words_to_remove))
## Reading data from CMS
Qs <- jsonlite::fromJSON('data/qs_topicwise.json')
Qs <- flatten(Qs)
#Qs <- read_tsv("qs_topicwise_dump.tsv")
head(Qs)
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
#dplyr::select(1:9) %>%                    # Keeping only the first 9 columns
#dplyr::filter(!is.na(difficulty))  %>%    # No need to clean overflow
dplyr::mutate(Grade = str_sub(topic_code, 5, 6), Subject = str_sub(topic_code, 1, 3), Curriculum = str_sub(topic_code, 8, 10), Ch_No = str_sub(topic_code, 12, 13))
repo <- Qs_clean %>% filter(Curriculum =="JEE", Subject == subject)
chapters_to_remove = c('Selection Test', 'Repository', 'Bridge Intervention Curriculum', 'M1.1 Scaffold test','Tally Marks')
chapters_with_no_data = c('Static Electricity', 'Experimental Skills', 'Principle of Mathematical Induction')
repo <- repo %>%
filter(!chapter %in%  chapters_to_remove) %>%
filter(!chapter %in%  chapters_with_no_data)
dummy_questions = which(grepl("DUMMY", repo$question_text))
repo <- repo[-dummy_questions]
repo %>% group_by(chapter) %>% summarize(count_qs =n())
repo<-repo[sample(nrow(repo)),]       ## Randomize row numbers
text_corpus <- VCorpus(VectorSource(repo$question_text))
lapply(text_corpus[2:4], as.character)       # Multiple docs
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
#text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))
# Remove one letter words
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern="\\b\\w{1,2}\\b", replacement = "")
## Add stopwords like left, right (frac ?)
text_corpus_clean <- tm_map(text_corpus_clean, removeWords, subject_words_to_remove)
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
#text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
text_corpus_clean <- tm_map(text_corpus_clean, stripWhitespace)
lapply(text_corpus_clean[2:4], as.character)
library('tidyverse')
library('e1071')
library('SparseM')
library('tm')
library('caret')
#library('SnowballC')
#library('wordcloud')
library('jsonlite')
library(reticulate)
path_to_python <- "/usr/local/bin/python3"
use_python(path_to_python)
knitr::knit_engines$set(python = reticulate::eng_python)
py_available(initialize = TRUE)
# Algorithm. Choices: svm, naiveBayes, etc
algorithm = naiveBayes
# Categorical or not. Choices: TRUE or FALSE
categorical = FALSE
# Weighting function. Choices: weightTf, weightTfIdf
# Whether to normalize terms of Document Matrix. Choices: TRUE, FALSE
weightingFunction = weightTf
normalize = FALSE
# Whether to write to db
writeToDb = TRUE
subject = "PHY"
words_to_remove = vector(mode="list", length=3)
names(words_to_remove) = c("PHY", "CHM", "MTH")
# MATH
words_to_remove[[1]] =  c("rightarrow", "hence", "frac", "text", "sqrt", "times", "value", "amp", "statement", "will", "equal", "number", "tan", "now", "can", "two", "get", "true", "lambda")
# CHM
words_to_remove[[2]] = c("following", "can", "correct", "hence", "amp", "text", "will", "reaction", "given", "frac", "rightarrow", "circ", "number", "matrix", "mol", "compound", "true", "one", "group", "solution")
# # PHY
words_to_remove[[3]] = c("frac", "given", "times", "will", "two", "rightarrow", "sqrt")
subject_words_to_remove = c(stopwords(), get(subject, words_to_remove))
## Reading data from CMS
Qs <- jsonlite::fromJSON('data/qs_topicwise.json')
Qs <- flatten(Qs)
#Qs <- read_tsv("qs_topicwise_dump.tsv")
head(Qs)
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
#dplyr::select(1:9) %>%                    # Keeping only the first 9 columns
#dplyr::filter(!is.na(difficulty))  %>%    # No need to clean overflow
dplyr::mutate(Grade = str_sub(topic_code, 5, 6), Subject = str_sub(topic_code, 1, 3), Curriculum = str_sub(topic_code, 8, 10), Ch_No = str_sub(topic_code, 12, 13))
repo <- Qs_clean %>% filter(Curriculum =="JEE", Subject == subject)
chapters_to_remove = c('Selection Test', 'Repository', 'Bridge Intervention Curriculum', 'M1.1 Scaffold test','Tally Marks')
chapters_with_no_data = c('Static Electricity', 'Experimental Skills', 'Principle of Mathematical Induction')
repo <- repo %>%
filter(!chapter %in%  chapters_to_remove) %>%
filter(!chapter %in%  chapters_with_no_data)
dummy_questions = which(grepl("DUMMY", repo$question_text))
repo <- repo[-dummy_questions]
repo %>% group_by(chapter) %>% summarize(count_qs =n())
repo<-repo[sample(nrow(repo)),]       ## Randomize row numbers
text_corpus <- VCorpus(VectorSource(repo$question_text))
lapply(text_corpus[2:4], as.character)       # Multiple docs
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
#text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))
# Remove one letter words
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern="\\b\\w{1,2}\\b", replacement = "")
## Add stopwords like left, right (frac ?)
text_corpus_clean <- tm_map(text_corpus_clean, removeWords, subject_words_to_remove)
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
#text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
text_corpus_clean <- tm_map(text_corpus_clean, stripWhitespace)
lapply(text_corpus_clean[2:4], as.character)
# TF-IDF
text_dtm <- DocumentTermMatrix(text_corpus_clean, control = list(stemming = TRUE))
# Add a dictionary to DTM ?
folds <- cut(seq(1,nrow(repo)),breaks=5,labels=FALSE)
test_indices = which(folds==1, arr.ind=TRUE)
text_dtm_train <- text_dtm[-test_indices,  ]
text_dtm_test <- text_dtm[test_indices,  ]
text_train_labels <- as.factor(repo[-test_indices,  ]$chapter)
text_test_labels <- as.factor(repo[test_indices,  ]$chapter)
round(prop.table(table(text_train_labels))*100,2)
round(prop.table(table(text_test_labels))*100,2)
## Filter features by selecting words appearing at least a specified number of times
text_freq_words <- findFreqTerms(text_dtm, 25)
# tf-idf ?
# Useful to remove words which maybe in training set but not in test set leading to an out of bounds error
text_dtm_freq_train <- text_dtm_train[ , text_freq_words]
text_dtm_freq_test <- text_dtm_test[ , text_freq_words]
convert_counts <- function(x) {
if(categorical) {
x <- ifelse(x > 0, "Yes", "No")
}
else {
x <- x
}
}
# MARGIN = 1 is used for rows
text_train <- apply(text_dtm_freq_train, MARGIN = 2, convert_counts)
text_test <- apply(text_dtm_freq_test, MARGIN = 2, convert_counts)
# Laplace - This allows words that did not appear earlier to have an indisputable say in the classification process. Just because the word "ringtone" only appeared in the spam messages in the training data, it does not mean that every message with this word should be classified as spam
text_classifier <- algorithm(as.matrix(text_train), text_train_labels)
rawResults <- predict(text_classifier,as.matrix(text_test), type="raw")
results <- as.factor( colnames(rawResults)[apply(rawResults, 1, which.max)] )
true_values <- repo[test_indices, ]$chapter
test_document <- repo[test_indices, ]$question_text
## as.table((test_document,true_values, results)) ??
conf = confusionMatrix(rawResults, text_test_labels)
