results <- predict(text_classifier,text_test)
confusionMatrix(results, text_test_labels)
#View(results)
library('tidyverse')
library('e1071')
library('SparseM')
library('tm')
library('caret')
library('SnowballC')
library('wordcloud')
# Reading entire Qs repo
Qs <- read_tsv("qs_topicwise_dump.tsv")
knitr::kable(head(Qs))
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
dplyr::select(1:10) %>%                    # Keeping only the first 10 columns
dplyr::filter(!is.na(Difficulty))  %>%           # Cleaning overflow
dplyr::mutate(Grade = str_sub(Topic_Code, 5, 6), Subject = str_sub(Topic_Code, 1, 3), Curriculum = str_sub(Topic_Code, 8, 10), Ch_No = str_sub(Topic_Code, 12, 13))
# Removing non-UTF characters
Qs_clean$Text <- lapply(Qs_clean$Text,gsub, pattern = "[^[:alnum:]]", replacement = " ")
Qs_clean$Text <- lapply(Qs_clean$Text,gsub, pattern = "<.*?>", replacement= " ")
knitr::kable(head(Qs_clean))
Qs_math <- Qs_clean %>% filter(Curriculum =="JEE", Subject == "MTH")
Qs_math %>% group_by(Chapter) %>% summarize(count_qs =n())
binary <-  filter(Qs_math, Chapter %in% c("Applications of Derivatives", "Fundamentals of Mathematics"))
# Using chapter names for filtering gave the error message :
# longer object length is not a multiple of shorter object length
#binary <- arrange(binary, Code)      ## To rearrange and randomize chapters by row no
binary %>% group_by(Chapter) %>% summarize(count_qs =n())
binary$Chapter <- factor(binary$Chapter)
str(binary$Chapter)
text_corpus <- VCorpus(VectorSource(binary$Text))
##include both and test and training set to build the corpus
#inspect (text_corpus)
lapply(text_corpus[2:4], as.character)       # Multiple docs
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))
## Add stopwords like left, right, frac
text_corpus_clean <- tm_map(text_corpus_clean, removeWords, c(stopwords(), "left","right"))
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
text_corpus_clean <- tm_map(text_corpus_clean, stripWhitespace)
lapply(text_corpus_clean[2:4], as.character)
text_dtm <- DocumentTermMatrix(text_corpus_clean)
text_dtm
# Add a dictionary to DTM ?
text_dtm_train <- text_dtm[binary$Difficulty == '2', ]
text_dtm_test <- text_dtm[binary$Difficulty != '2', ]
text_train_labels <- binary[binary$Difficulty == '2', ]$Chapter
text_test_labels <- binary[binary$Difficulty != '2', ]$Chapter
prop.table(table(text_train_labels))
prop.table(table(text_test_labels))
wordcloud(text_corpus_clean, min.freq=10, random.order = FALSE)
wordcloud(text_corpus_clean[binary$Chapter == "Applications of Derivatives"], max.words = 40, scale = c(3, 0.5))
wordcloud(text_corpus_clean[binary$Chapter == "Fundamentals of Mathematics"], max.words = 40, scale = c(3, 0.5))
text_freq_words <- findFreqTerms(text_dtm, 25)
# tf-idf ?
# Useful to remove words which maybe in training set but not in test set leading to an out of bounds error
text_dtm_freq_train <- text_dtm_train[ , text_freq_words]
text_dtm_freq_test <- text_dtm_test[ , text_freq_words]
convert_counts <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
# MARGIN = 1 is used for rows
text_train <- apply(text_dtm_freq_train, MARGIN = 2, convert_counts)
text_test <- apply(text_dtm_freq_test, MARGIN = 2, convert_counts)
str(text_train)
str(text_test)
text_classifier <- naiveBayes(text_train, text_train_labels, laplace = 1)
# Laplace - This allows words that did not appear earlier to have an indisputable say in the classification process. Just because the word "ringtone" only appeared in the spam messages in the training data, it does not mean that every message with this word should be classified as spam
#results <- predict(text_classifier,as.matrix(text_dtm_test))
results <- predict(text_classifier,text_test)
confusionMatrix(results, text_test_labels)
#View(results)
library('tidyverse')
library('e1071')
library('SparseM')
library('tm')
library('caret')
library('SnowballC')
library('wordcloud')
# Reading entire Qs repo
Qs <- read_tsv("qs_topicwise_dump.tsv")
knitr::kable(head(Qs))
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
dplyr::select(1:10) %>%                    # Keeping only the first 10 columns
dplyr::filter(!is.na(Difficulty))  %>%           # Cleaning overflow
dplyr::mutate(Grade = str_sub(Topic_Code, 5, 6), Subject = str_sub(Topic_Code, 1, 3), Curriculum = str_sub(Topic_Code, 8, 10), Ch_No = str_sub(Topic_Code, 12, 13))
# Removing non-UTF characters
Qs_clean$Text <- lapply(Qs_clean$Text,gsub, pattern = "[^[:alnum:]]", replacement = " ")
Qs_clean$Text <- lapply(Qs_clean$Text,gsub, pattern = "<.*?>", replacement= " ")
knitr::kable(head(Qs_clean))
Qs_math <- Qs_clean %>% filter(Curriculum =="JEE", Subject == "MTH")
Qs_math %>% group_by(Chapter) %>% summarize(count_qs =n())
binary <-  filter(Qs_math, Chapter %in% c("Applications of Derivatives", "Fundamentals of Mathematics"))
# Using chapter names for filtering gave the error message :
# longer object length is not a multiple of shorter object length
#binary <- arrange(binary, Code)      ## To rearrange and randomize chapters by row no
binary %>% group_by(Chapter) %>% summarize(count_qs =n())
binary$Chapter <- factor(binary$Chapter)
str(binary$Chapter)
text_corpus <- VCorpus(VectorSource(binary$Text))
##include both and test and training set to build the corpus
#inspect (text_corpus)
lapply(text_corpus[2:4], as.character)       # Multiple docs
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))
## Add stopwords like left, right (frac ?)
text_corpus_clean <- tm_map(text_corpus_clean, removeWords, c(stopwords(), "left","right", "frac"))
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
text_corpus_clean <- tm_map(text_corpus_clean, stripWhitespace)
lapply(text_corpus_clean[2:4], as.character)
text_dtm <- DocumentTermMatrix(text_corpus_clean)
text_dtm
# Add a dictionary to DTM ?
text_dtm_train <- text_dtm[binary$Difficulty == '2', ]
text_dtm_test <- text_dtm[binary$Difficulty != '2', ]
text_train_labels <- binary[binary$Difficulty == '2', ]$Chapter
text_test_labels <- binary[binary$Difficulty != '2', ]$Chapter
prop.table(table(text_train_labels))
prop.table(table(text_test_labels))
wordcloud(text_corpus_clean, min.freq=10, random.order = FALSE)
wordcloud(text_corpus_clean[binary$Chapter == "Applications of Derivatives"], max.words = 40, scale = c(3, 0.5))
wordcloud(text_corpus_clean[binary$Chapter == "Fundamentals of Mathematics"], max.words = 40, scale = c(3, 0.5))
text_freq_words <- findFreqTerms(text_dtm, 25)
# tf-idf ?
# Useful to remove words which maybe in training set but not in test set leading to an out of bounds error
text_dtm_freq_train <- text_dtm_train[ , text_freq_words]
text_dtm_freq_test <- text_dtm_test[ , text_freq_words]
convert_counts <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
# MARGIN = 1 is used for rows
text_train <- apply(text_dtm_freq_train, MARGIN = 2, convert_counts)
text_test <- apply(text_dtm_freq_test, MARGIN = 2, convert_counts)
str(text_train)
str(text_test)
text_classifier <- naiveBayes(text_train, text_train_labels, laplace = 1)
# Laplace - This allows words that did not appear earlier to have an indisputable say in the classification process. Just because the word "ringtone" only appeared in the spam messages in the training data, it does not mean that every message with this word should be classified as spam
#results <- predict(text_classifier,as.matrix(text_dtm_test))
results <- predict(text_classifier,text_test)
confusionMatrix(results, text_test_labels)
#View(results)
library('tidyverse')
library('e1071')
library('SparseM')
library('tm')
library('caret')
library('SnowballC')
library('wordcloud')
# Reading entire Qs repo
Qs <- read_tsv("qs_topicwise_dump.tsv")
knitr::kable(head(Qs))
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
dplyr::select(1:10) %>%                    # Keeping only the first 10 columns
dplyr::filter(!is.na(Difficulty))  %>%           # Cleaning overflow
dplyr::mutate(Grade = str_sub(Topic_Code, 5, 6), Subject = str_sub(Topic_Code, 1, 3), Curriculum = str_sub(Topic_Code, 8, 10), Ch_No = str_sub(Topic_Code, 12, 13))
# Removing non-UTF characters
Qs_clean$Text <- lapply(Qs_clean$Text,gsub, pattern = "[^[:alnum:]]", replacement = " ")
Qs_clean$Text <- lapply(Qs_clean$Text,gsub, pattern = "<.*?>", replacement= " ")
knitr::kable(head(Qs_clean))
Qs_math <- Qs_clean %>% filter(Curriculum =="JEE", Subject == "MTH")
Qs_math %>% group_by(Chapter) %>% summarize(count_qs =n())
binary <-  filter(Qs_math, Chapter %in% c("Applications of Derivatives", "Fundamentals of Mathematics"))
# Using chapter names for filtering gave the error message :
# longer object length is not a multiple of shorter object length
#binary <- arrange(binary, Code)      ## To rearrange and randomize chapters by row no
binary %>% group_by(Chapter) %>% summarize(count_qs =n())
binary$Chapter <- factor(binary$Chapter)
str(binary$Chapter)
text_corpus <- VCorpus(VectorSource(binary$Text))
##include both and test and training set to build the corpus
#inspect (text_corpus)
lapply(text_corpus[2:4], as.character)       # Multiple docs
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))
## Add stopwords like left, right (frac ?)
text_corpus_clean <- tm_map(text_corpus_clean, removeWords, c(stopwords(), "left","right"))
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
text_corpus_clean <- tm_map(text_corpus_clean, stripWhitespace)
lapply(text_corpus_clean[2:4], as.character)
text_dtm <- DocumentTermMatrix(text_corpus_clean)
text_dtm
# Add a dictionary to DTM ?
text_dtm_train <- text_dtm[binary$Difficulty == '2', ]
text_dtm_test <- text_dtm[binary$Difficulty != '2', ]
text_train_labels <- binary[binary$Difficulty == '2', ]$Chapter
text_test_labels <- binary[binary$Difficulty != '2', ]$Chapter
prop.table(table(text_train_labels))
prop.table(table(text_test_labels))
wordcloud(text_corpus_clean, min.freq=10, random.order = FALSE)
wordcloud(text_corpus_clean[binary$Chapter == "Applications of Derivatives"], max.words = 40, scale = c(3, 0.5))
wordcloud(text_corpus_clean[binary$Chapter == "Fundamentals of Mathematics"], max.words = 40, scale = c(3, 0.5))
text_freq_words <- findFreqTerms(text_dtm, 25)
# tf-idf ?
# Useful to remove words which maybe in training set but not in test set leading to an out of bounds error
text_dtm_freq_train <- text_dtm_train[ , text_freq_words]
text_dtm_freq_test <- text_dtm_test[ , text_freq_words]
convert_counts <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
# MARGIN = 1 is used for rows
text_train <- apply(text_dtm_freq_train, MARGIN = 2, convert_counts)
text_test <- apply(text_dtm_freq_test, MARGIN = 2, convert_counts)
str(text_train)
str(text_test)
text_classifier <- naiveBayes(text_train, text_train_labels, laplace = 1)
# Laplace - This allows words that did not appear earlier to have an indisputable say in the classification process. Just because the word "ringtone" only appeared in the spam messages in the training data, it does not mean that every message with this word should be classified as spam
#results <- predict(text_classifier,as.matrix(text_dtm_test))
results <- predict(text_classifier,text_test)
confusionMatrix(results, text_test_labels)
#View(results)
library('tidyverse')
library('e1071')
library('SparseM')
library('tm')
library('caret')
library('SnowballC')
library('wordcloud')
# Reading entire Qs repo
Qs <- read_tsv("qs_topicwise_dump.tsv")
knitr::kable(head(Qs))
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
dplyr::select(1:10) %>%                    # Keeping only the first 10 columns
dplyr::filter(!is.na(Difficulty))  %>%           # Cleaning overflow
dplyr::mutate(Grade = str_sub(Topic_Code, 5, 6), Subject = str_sub(Topic_Code, 1, 3), Curriculum = str_sub(Topic_Code, 8, 10), Ch_No = str_sub(Topic_Code, 12, 13))
# Removing non-UTF characters
Qs_clean$Text <- lapply(Qs_clean$Text,gsub, pattern = "[^[:alnum:]]", replacement = " ")
Qs_clean$Text <- lapply(Qs_clean$Text,gsub, pattern = "<.*?>", replacement= " ")
knitr::kable(head(Qs_clean))
Qs_math <- Qs_clean %>% filter(Curriculum =="JEE", Subject == "MTH")
Qs_math %>% group_by(Chapter) %>% summarize(count_qs =n())
binary <-  filter(Qs_math, Chapter %in% c("Applications of Derivatives", "Fundamentals of Mathematics", "Conic Sections - I"))
# Using chapter names for filtering gave the error message :
# longer object length is not a multiple of shorter object length
#binary <- arrange(binary, Code)      ## To rearrange and randomize chapters by row no
binary %>% group_by(Chapter) %>% summarize(count_qs =n())
multi <-  filter(Qs_math, Chapter %in% c("Applications of Derivatives", "Fundamentals of Mathematics", "Conic Sections - I"))
# Using chapter names for filtering gave the error message :
# longer object length is not a multiple of shorter object length
#binary <- arrange(binary, Code)      ## To rearrange and randomize chapters by row no
multi %>% group_by(Chapter) %>% summarize(count_qs =n())
multi$Chapter <- factor(multi$Chapter)
str(multi$Chapter)
text_corpus <- VCorpus(VectorSource(multi$Text))
##include both and test and training set to build the corpus
#inspect (text_corpus)
lapply(text_corpus[2:4], as.character)       # Multiple docs
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))
## Add stopwords like left, right (frac ?)
text_corpus_clean <- tm_map(text_corpus_clean, removeWords, c(stopwords(), "left","right"))
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
text_corpus_clean <- tm_map(text_corpus_clean, stripWhitespace)
lapply(text_corpus_clean[2:4], as.character)
text_dtm <- DocumentTermMatrix(text_corpus_clean)
text_dtm
# Add a dictionary to DTM ?
text_dtm_train <- text_dtm[multi$Difficulty == '2', ]
text_dtm_test <- text_dtm[multi$Difficulty != '2', ]
text_train_labels <- multi[multi$Difficulty == '2', ]$Chapter
text_test_labels <- multi[multi$Difficulty != '2', ]$Chapter
prop.table(table(text_train_labels))
prop.table(table(text_test_labels))
wordcloud(text_corpus_clean, min.freq=10, random.order = FALSE)
wordcloud(text_corpus_clean[multi$Chapter == "Applications of Derivatives"], max.words = 40, scale = c(3, 0.5))
wordcloud(text_corpus_clean[multi$Chapter == "Fundamentals of Mathematics"], max.words = 40, scale = c(3, 0.5))
wordcloud(text_corpus_clean[multi$Chapter == "Conic Sections - I"], max.words = 40, scale = c(3, 0.5))
text_freq_words <- findFreqTerms(text_dtm, 25)
# tf-idf ?
# Useful to remove words which maybe in training set but not in test set leading to an out of bounds error
text_dtm_freq_train <- text_dtm_train[ , text_freq_words]
text_dtm_freq_test <- text_dtm_test[ , text_freq_words]
text_freq_words <- findFreqTerms(text_dtm, 25)
# tf-idf ?
# Useful to remove words which maybe in training set but not in test set leading to an out of bounds error
text_dtm_freq_train <- text_dtm_train[ , text_freq_words]
text_dtm_freq_test <- text_dtm_test[ , text_freq_words]
convert_counts <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
# MARGIN = 1 is used for rows
text_train <- apply(text_dtm_freq_train, MARGIN = 2, convert_counts)
text_test <- apply(text_dtm_freq_test, MARGIN = 2, convert_counts)
str(text_train)
str(text_test)
text_classifier <- naiveBayes(text_train, text_train_labels, laplace = 1)
# Laplace - This allows words that did not appear earlier to have an indisputable say in the classification process. Just because the word "ringtone" only appeared in the spam messages in the training data, it does not mean that every message with this word should be classified as spam
#results <- predict(text_classifier,as.matrix(text_dtm_test))
results <- predict(text_classifier,text_test)
confusionMatrix(results, text_test_labels)
#View(results)
Qs_math <- Qs_clean %>% filter(Curriculum =="JEE", Subject == "PHY")
Qs_math %>% group_by(Chapter) %>% summarize(count_qs =n())
Qs_math <- Qs_clean %>% filter(Curriculum =="JEE", Subject == "CHM")
Qs_math %>% group_by(Chapter) %>% summarize(count_qs =n())
multi <-  filter(Qs_math, Chapter %in% c("Aldehydes and Ketones", "Coordination Compounds", "Biomolecules, Polymers and Chemistry in Everyday life", "Electrochemistry", "Amines"))
# Using chapter names for filtering gave the error message :
# longer object length is not a multiple of shorter object length
#multi <- arrange(multi, Code)      ## To rearrange and randomize chapters by row no
multi %>% group_by(Chapter) %>% summarize(count_qs =n())
multi$Chapter <- factor(multi$Chapter)
str(multi$Chapter)
text_corpus <- VCorpus(VectorSource(multi$Text))
##include both and test and training set to build the corpus
#inspect (text_corpus)
lapply(text_corpus[2:4], as.character)       # Multiple docs
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))
## Add stopwords like left, right (frac ?)
text_corpus_clean <- tm_map(text_corpus_clean, removeWords, c(stopwords(), "left","right"))
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))
## Add stopwords like left, right (frac ?)
text_corpus_clean <- tm_map(text_corpus_clean, removeWords, c(stopwords(), "left","right"))
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
#text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
text_corpus_clean <- tm_map(text_corpus_clean, stripWhitespace)
lapply(text_corpus_clean[2:4], as.character)
text_dtm <- DocumentTermMatrix(text_corpus_clean)
library('tidyverse')
library('e1071')
library('SparseM')
library('tm')
library('caret')
library('SnowballC')
library('wordcloud')
# Reading entire Qs repo
Qs <- read_tsv("qs_topicwise_dump.tsv")
knitr::kable(head(Qs))
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
dplyr::select(1:10) %>%                    # Keeping only the first 10 columns
dplyr::filter(!is.na(Difficulty))  %>%           # Cleaning overflow
dplyr::mutate(Grade = str_sub(Topic_Code, 5, 6), Subject = str_sub(Topic_Code, 1, 3), Curriculum = str_sub(Topic_Code, 8, 10), Ch_No = str_sub(Topic_Code, 12, 13))
# Removing non-UTF characters
Qs_clean$Text <- lapply(Qs_clean$Text,gsub, pattern = "[^[:alnum:]]", replacement = " ")
Qs_clean$Text <- lapply(Qs_clean$Text,gsub, pattern = "<.*?>", replacement= " ")
knitr::kable(head(Qs_clean))
Qs_math <- Qs_clean %>% filter(Curriculum =="JEE", Subject == "CHM")
Qs_math %>% group_by(Chapter) %>% summarize(count_qs =n())
multi <-  filter(Qs_math, Chapter %in% c("Aldehydes and Ketones", "Coordination Compounds", "Biomolecules, Polymers and Chemistry in Everyday life", "Electrochemistry", "Amines"))
# Using chapter names for filtering gave the error message :
# longer object length is not a multiple of shorter object length
#multi <- arrange(multi, Code)      ## To rearrange and randomize chapters by row no
multi %>% group_by(Chapter) %>% summarize(count_qs =n())
multi$Chapter <- factor(multi$Chapter)
str(multi$Chapter)
text_corpus <- VCorpus(VectorSource(multi$Text))
##include both and test and training set to build the corpus
#inspect (text_corpus)
lapply(text_corpus[2:4], as.character)       # Multiple docs
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))
## Add stopwords like left, right (frac ?)
text_corpus_clean <- tm_map(text_corpus_clean, removeWords, c(stopwords(), "left","right"))
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
library('tidyverse')
library('e1071')
library('SparseM')
library('tm')
library('caret')
library('SnowballC')
library('wordcloud')
# Reading entire Qs repo
Qs <- read_tsv("qs_topicwise_dump.tsv")
knitr::kable(head(Qs))
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
dplyr::select(1:10) %>%                    # Keeping only the first 10 columns
dplyr::filter(!is.na(Difficulty))  %>%           # Cleaning overflow
dplyr::mutate(Grade = str_sub(Topic_Code, 5, 6), Subject = str_sub(Topic_Code, 1, 3), Curriculum = str_sub(Topic_Code, 8, 10), Ch_No = str_sub(Topic_Code, 12, 13))
# Removing non-UTF characters
Qs_clean$Text <- lapply(Qs_clean$Text,gsub, pattern = "[^[:alnum:]]", replacement = " ")
Qs_clean$Text <- lapply(Qs_clean$Text,gsub, pattern = "<.*?>", replacement= " ")
knitr::kable(head(Qs_clean))
Qs_math <- Qs_clean %>% filter(Curriculum =="JEE", Subject == "CHM")
Qs_math %>% group_by(Chapter) %>% summarize(count_qs =n())
multi <-  filter(Qs_math, Chapter %in% c("Aldehydes and Ketones", "Coordination Compounds", "Biomolecules, Polymers and Chemistry in Everyday life", "Electrochemistry", "Amines"))
# Using chapter names for filtering gave the error message :
# longer object length is not a multiple of shorter object length
#multi <- arrange(multi, Code)      ## To rearrange and randomize chapters by row no
multi %>% group_by(Chapter) %>% summarize(count_qs =n())
multi$Chapter <- factor(multi$Chapter)
str(multi$Chapter)
text_corpus <- VCorpus(VectorSource(multi$Text))
##include both and test and training set to build the corpus
#inspect (text_corpus)
lapply(text_corpus[2:4], as.character)       # Multiple docs
View(Qs_math)
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))
text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
library('tidyverse')
library('e1071')
library('SparseM')
library('tm')
library('caret')
library('SnowballC')
library('wordcloud')
# Reading entire Qs repo
Qs <- read_tsv("qs_topicwise_dump.tsv")
knitr::kable(head(Qs))
View(Qs)
# Need to clean the overflow of text
Qs_clean <- Qs  %>%
dplyr::select(1:10) %>%                    # Keeping only the first 10 columns
dplyr::filter(!is.na(Difficulty))  %>%           # Cleaning overflow
dplyr::mutate(Grade = str_sub(Topic_Code, 5, 6), Subject = str_sub(Topic_Code, 1, 3), Curriculum = str_sub(Topic_Code, 8, 10), Ch_No = str_sub(Topic_Code, 12, 13))
# Removing non-UTF characters
Qs_clean$Text <- lapply(Qs_clean$Text,gsub, pattern = "[^[:alnum:]]", replacement = " ")
Qs_clean$Text <- lapply(Qs_clean$Text,gsub, pattern = "<.*?>", replacement= " ")
knitr::kable(head(Qs_clean))
View(Qs_clean)
multi <-  filter(Qs_math, Chapter %in% c("Aldehydes and Ketones", "Coordination Compounds", "Biomolecules, Polymers and Chemistry in Everyday life", "Electrochemistry", "Amines"))
Qs_math <- Qs_clean %>% filter(Curriculum =="JEE", Subject == "MTH")
Qs_math %>% group_by(Chapter) %>% summarize(count_qs =n())
multi <-  filter(Qs_math, Chapter %in% c("Aldehydes and Ketones", "Coordination Compounds", "Biomolecules, Polymers and Chemistry in Everyday life", "Electrochemistry", "Amines"))
# Using chapter names for filtering gave the error message :
# longer object length is not a multiple of shorter object length
#multi <- arrange(multi, Code)      ## To rearrange and randomize chapters by row no
multi %>% group_by(Chapter) %>% summarize(count_qs =n())
Qs_math <- Qs_clean %>% filter(Curriculum =="JEE", Subject == "CHM")
Qs_math %>% group_by(Chapter) %>% summarize(count_qs =n())
multi <-  filter(Qs_math, Chapter %in% c("Aldehydes and Ketones", "Coordination Compounds", "Biomolecules, Polymers and Chemistry in Everyday life", "Electrochemistry", "Amines"))
# Using chapter names for filtering gave the error message :
# longer object length is not a multiple of shorter object length
#multi <- arrange(multi, Code)      ## To rearrange and randomize chapters by row no
multi %>% group_by(Chapter) %>% summarize(count_qs =n())
multi$Chapter <- factor(multi$Chapter)
str(multi$Chapter)
Chapter
multi$Chapter
factor(multi$Chapter)
str(multi$Chapter)
?str
factor(multi$Chapter)
factor(multi$Chapter).getNamespace()
View(multi)
text_corpus <- VCorpus(VectorSource(multi$Text))
##include both and test and training set to build the corpus
#inspect (text_corpus)
lapply(text_corpus[2:4], as.character)       # Multiple docs
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))
## Add stopwords like left, right (frac ?)
text_corpus_clean <- tm_map(text_corpus_clean, removeWords, c(stopwords(), "left","right"))
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
# Source : [2] Removing non-UTF8 characters
text_corpus_clean <- tm_map(text_corpus, content_transformer(gsub), pattern ="[^[:alnum:]]" , replacement = " ")
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(gsub), pattern ="[\u0080-\uffff]" , replacement = " ")
## Now non-UTF characters are removed. We can do regular tasks on the clean corpus.
text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
text_corpus_clean <- tm_map(text_corpus_clean, content_transformer(tolower))
## Add stopwords like left, right (frac ?)
text_corpus_clean <- tm_map(text_corpus_clean, removeWords, c(stopwords(), "left","right"))
text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
#text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
text_corpus_clean <- tm_map(text_corpus_clean, stripWhitespace)
lapply(text_corpus_clean[2:4], as.character)
text_dtm <- DocumentTermMatrix(text_corpus_clean)
